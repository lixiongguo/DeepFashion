{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现的问题汇总：\n",
    "1. ROI pooling自定义层难点：\n",
    "\n",
    "    faster rcnn中的该层使用的都是单张图片输入，每张图片读取一个多个ROI，一次只读取一张图片。但是fashionNet需要每张图片读取8个固定的ROI，且一次读取多张图片\n",
    "\n",
    "    解决办法：在ROI中使用`batches = K.shape(box)[0];box_ind = K.arange(batches)`获取batch大小，从而实现读取多张图片，但似乎会导致内存溢出。目前好的解决办法\n",
    "    \n",
    "    \n",
    "2. 关于内存溢出（OOM）的一种解决办法：\n",
    "\n",
    "    fashionnet引入了triplet loss来控制类之间的距离，但是一次需要同时使用三张图片输入，加上模型为深层网络，容易造成内存溢出（当前使用batches=1也不行）。\n",
    "    \n",
    "    解决办法：可以尝试拆除模型triplet loss。当前版本已经拆除triplet loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:24.653589Z",
     "start_time": "2019-04-12T00:46:21.990885Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The Code reproduced from the Fashion Net paper\n",
    "# \n",
    "# Author: Jasonsey\n",
    "# Email: 2627866800@qq.com\n",
    "# \n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  # which gpu to use\n",
    "from pathlib import Path\n",
    "import six\n",
    "import math\n",
    "import itertools\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, backend, models, utils\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "# from keras_contrib.applications import ResNet\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, LambdaCallback\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras_applications\n",
    "from keras_applications.resnet import ResNet50, ResNet101, ResNet152\n",
    "from keras.applications import VGG16\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile\n",
    "from cv2 import cv2\n",
    "# from lapjv import lapjv    # if use conda，need to install from lapjv's git repo\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:25.140939Z",
     "start_time": "2019-04-12T00:46:24.658199Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    1: 'fashionnet',\n",
    "    2: 'resnet34'\n",
    "}\n",
    "no_model = 1\n",
    "\n",
    "\n",
    "TRAIN_DF = '../data/input/train.csv'\n",
    "VAL_DF = '../data/input/val.csv'\n",
    "TEST_DF = '../data/input/test.csv'\n",
    "\n",
    "MODEL_PATH = '../data/output/models/fashionnet/'\n",
    "LOGGER_PATH = '../data/output/logs/fashionnet/'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DF).fillna(0)\n",
    "val_df = pd.read_csv(VAL_DF).fillna(0)\n",
    "test_df = pd.read_csv(TEST_DF).fillna(0)\n",
    "\n",
    "\n",
    "image_shape = (224, 224, 3)  # all images will be adjusted to this shape\n",
    "num_category = 23  # all images belong to 23 category\n",
    "num_attr = 463  # each image has 463 attribute \n",
    "num_landmark_visibility = 3  # landmark has 3 status: visible, unvisible, unsure\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:25.163000Z",
     "start_time": "2019-04-12T00:46:25.144016Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_path(p, **kwargs):\n",
    "    \"\"\"read the complete path\n",
    "    \n",
    "    Arguments\n",
    "        p: the string of path that save in csv files, such as `img/MEN/Denim/id_00000080/01_1_front.jpg`\n",
    "        \n",
    "    Return:\n",
    "        the complete path, such as `../data/input/Img/img/MEN/Denim/id_00000080/01_1_front.jpg`\n",
    "    \"\"\"\n",
    "    header = Path('../data/input/Img/')\n",
    "    return str(header / p)\n",
    "\n",
    "\n",
    "def read_raw_image(p, **kwargs):\n",
    "    \"\"\"read image from disk\n",
    "    \n",
    "    Arguments\n",
    "        p: the string of path that save in csv files, such as `img/MEN/Denim/id_00000080/01_1_front.jpg`\n",
    "    \n",
    "    Return\n",
    "        The pil image object\n",
    "    \"\"\"\n",
    "    img = Image.open(expand_path(p))\n",
    "    return img\n",
    "\n",
    "\n",
    "def show_img(imgs, per_row=2, rescale=False):\n",
    "    \"\"\"show images in cell's output\n",
    "    \n",
    "    Arguments\n",
    "        imgs: a list of np.ndarray or pil.Image.Image, the length of imgs should no less than 2\n",
    "        per_row: how manny images are showed per row\n",
    "        rescale: if the images have been rescale into 0~1, then the rescale should be True\n",
    "    \n",
    "    Return\n",
    "        None       \n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for i in range(len(imgs)):\n",
    "        if isinstance(imgs[i], np.ndarray):\n",
    "            if rescale:\n",
    "                mm, mn = imgs[i].max(), imgs[i].min()\n",
    "                scale = 255 / (mm-mn)\n",
    "                imgs[i] += mn\n",
    "                imgs[i] *= scale\n",
    "            images.append(Image.fromarray(np.uint8(imgs[i])))\n",
    "        else:\n",
    "            images.append(imgs[i])\n",
    "    imgs = images\n",
    "    n = len(imgs)\n",
    "    rows = (n + per_row - 1) // per_row\n",
    "    cols = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(24 // per_row * cols, 24 // per_row * rows))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i, (img, ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:25.606919Z",
     "start_time": "2019-04-12T00:46:25.166988Z"
    }
   },
   "outputs": [],
   "source": [
    "# show 3 of the images in the train_df\n",
    "imgs = []\n",
    "for k in train_df['image_name']:\n",
    "    im = read_raw_image(k)\n",
    "    imgs.append(im)\n",
    "    if len(imgs) == 3:\n",
    "        break\n",
    "        \n",
    "show_img(imgs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:25.893178Z",
     "start_time": "2019-04-12T00:46:25.610241Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the category distribution\n",
    "_ = plt.hist(train_df['category_label'], bins=num_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataAgu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:25.905949Z",
     "start_time": "2019-04-12T00:46:25.895240Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aug(rate=0.8):\n",
    "    \"\"\"data set augmentation tool.\n",
    "    \n",
    "    The official documents: https://github.com/aleju/imgaug\n",
    "    \n",
    "    Arguments\n",
    "        rate: the rate to use data set augmentation tool. if 1, all of the images will use data \n",
    "            set augmentation tool, if 0, none of the images will use data set augmentation tool\n",
    "    \n",
    "    Return\n",
    "        The configured data set augmentation tool\n",
    "    \"\"\"\n",
    "    sometimes = lambda aug: iaa.Sometimes(rate, aug)\n",
    "    seq = iaa.Sequential([\n",
    "        sometimes([\n",
    "            iaa.Fliplr(0.5),  # 水平翻转\n",
    "            iaa.Flipud(0.5),  # 垂直翻转\n",
    "            iaa.Crop(percent=(0, 0.15)),  # 随机剪切像素，并保证shape不变\n",
    "            iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.05))),  # 高斯模糊\n",
    "            iaa.ContrastNormalization((0.75, 1.5)),  # 减弱或加强对比度\n",
    "            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),  # 高斯噪点\n",
    "            iaa.Multiply((0.9, 1.1), per_channel=0.2),  # 随机变换色调\n",
    "            iaa.Sometimes(0.6, iaa.Affine(\n",
    "                scale={\"x\": (0.9, 1.1), \"y\": (0.8, 1.2)},\n",
    "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "                rotate=(-15, 15),\n",
    "                shear=(-8, 8)\n",
    "            ))\n",
    "        ]),\n",
    "        iaa.Resize({'height': image_shape[0], 'width': image_shape[1]})\n",
    "    ], random_order=True)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def normal_image(img):\n",
    "    \"\"\"normalize an image by the Image Net's official preprocessing tool. \n",
    "    \n",
    "    Arguments\n",
    "        img: np.ndarray of pil.Image.Image\n",
    "        \n",
    "    Return\n",
    "        the normalized image\n",
    "    \"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = np.array(img, K.floatx())\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "身高1.8，家住铜锣湾。\n"
     ]
    }
   ],
   "source": [
    "print('身高{0}，家住{1}。'.format(1.8, '铜锣湾'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集生成器，保证图片在训练过程中的供给"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:25.926859Z",
     "start_time": "2019-04-12T00:46:25.907908Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ab912f5262d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTrainSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"the train sequence that inherit from `Seqence` \n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequence' is not defined"
     ]
    }
   ],
   "source": [
    "class TrainSequence(Sequence):\n",
    "    \"\"\"the train sequence that inherit from `Seqence` \n",
    "    \n",
    "    Arguments\n",
    "        data: a pd.DataFrame of train_df, val_df or test_df\n",
    "        batch_size：the batch size to fit the model\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, batch_size=32):\n",
    "        \"\"\"耗时：1m 22s\"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.data =data\n",
    "        self.seq = get_aug(0.8)\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "#         self.mlb.classes_ = list(range(num_attr))  # 总共有463个属性\n",
    "        self.mlb.fit([list(range(463))])\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"耗时：265ms\"\"\"\n",
    "        start = self.batch_size * index\n",
    "        end = self.batch_size * (index + 1)-1\n",
    "        batches = self.data.loc[start:end]\n",
    "        seq = self.seq.to_deterministic()\n",
    "        \n",
    "        # 读取源数据\n",
    "        xs_a, attrs, cates, lands_v, lands_local = [], [], [], [], []\n",
    "        for i in range(len(batches)):\n",
    "            # read image\n",
    "            a = batches.iloc[i]\n",
    "            x_a = normal_image(np.asarray(read_raw_image(a['image_name']).convert('RGB')))\n",
    "            xs_a.append(x_a)\n",
    "            \n",
    "            # read attr, when a['attribute_labels'] has single label, it will be read as an int\n",
    "            attr = [int(at) for at in str(a['attribute_labels']).split(' ') if at != '']\n",
    "            attrs.append(attr)\n",
    "            # read category\n",
    "            cates.append(a['category_label'])\n",
    "            # read landmark\n",
    "            lands_v.extend([a['landmark_visibility_{0}'.format(ii)] for ii in range(1, 9)])\n",
    "            lands_local.append(ia.KeypointsOnImage([\n",
    "                ia.Keypoint(\n",
    "                    x=a['landmark_location_x_{0}'.format(ii)],\n",
    "                    y=a['landmark_location_y_{0}'.format(ii)]\n",
    "                ) for ii in range(1, 9)\n",
    "            ], shape=x_a.shape))\n",
    "            \n",
    "        # aug image\n",
    "        xs_a = np.array(seq.augment_images(xs_a))\n",
    "        # aug landmark   \n",
    "        lands_v = to_categorical(lands_v, num_landmark_visibility)\n",
    "        lands_local = seq.augment_keypoints(lands_local)\n",
    "        res = []\n",
    "        for local in lands_local:\n",
    "            for keypoint in local.keypoints:\n",
    "                res.append([keypoint.x, keypoint.y])\n",
    "        lands_local = res\n",
    "        try:\n",
    "            lands_local = np.array(lands_local) / image_shape[:2]  # 缩放比例到0-1\n",
    "        except ValueError as e:\n",
    "            # 检查过所有数据输入都不能存在空值，但是仍然出现shape不一致，这里检查那个数据异常\n",
    "#             raise ValueError(f\"Error: {e}, \\n image: {batches['image_name']}'， \\n lands: {lands_local}\")\n",
    "            print(\"Value Error\")\n",
    "        lands = np.concatenate((lands_v, lands_local), axis=1)\n",
    "        # from (None*8, 5) to (None, 8*5)\n",
    "        lands = lands.reshape((len(batches), -1))\n",
    "        \n",
    "        # transform category\n",
    "        cates = to_categorical(cates, num_category)\n",
    "        # transsform attrs\n",
    "        attrs = self.mlb.transform(attrs)\n",
    "\n",
    "        return xs_a, [cates, lands, cates, attrs]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"耗时：1m 3.57s\"\"\"\n",
    "        self.data = self.data.sample(frac=1.0).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:26.081396Z",
     "start_time": "2019-04-12T00:46:25.928586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.5/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    }
   ],
   "source": [
    "train = TrainSequence(data=train_df, batch_size=6)\n",
    "xs_a, [cates, lands, cates, attrs] = train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T08:16:20.633845Z",
     "start_time": "2019-04-01T08:16:19.777682Z"
    }
   },
   "source": [
    "```python\n",
    "# the code to test the TrainSequence\n",
    "train = TrainSequence(data=train_df, batch_size=6)\n",
    "xs_a, [cates, lands, cates, attrs] = train[0]\n",
    "\n",
    "lands_local = lands.reshape((-1, 5))[:, 3:].reshape((-1, 8, 2))\n",
    "imgs = []\n",
    "for i in range(len(xs_a)):\n",
    "    keypoints = ia.KeypointsOnImage([ia.Keypoint(x=x[0], y=x[1]) for x in lands_local[i]], shape=xs_a[i].shape)\n",
    "    imgs.append(keypoints.draw_on_image(xs_a[i], color=(255, 0, 0), size=7))\n",
    "show_img(imgs, 3, rescale=False); lands\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:26.088579Z",
     "start_time": "2019-04-12T00:46:26.083545Z"
    }
   },
   "outputs": [],
   "source": [
    "class ValSequence(TrainSequence):\n",
    "    \"\"\"the val sequence that inherit from `TrainSeqence`\n",
    "    \n",
    "    Arguments\n",
    "        data: a pd.DataFrame of train_df, val_df or test_df\n",
    "        batch_size：the batch size to fit the model\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, batch_size=32):\n",
    "        \"\"\"耗时：1m 22s\"\"\"\n",
    "        super().__init__(data=data, batch_size=batch_size)\n",
    "        # make sure donot use data set augmentation by setting rate=0\n",
    "        self.seq = get_aug(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:26.099449Z",
     "start_time": "2019-04-12T00:46:26.090950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     7319\n",
       "9     4046\n",
       "12    3464\n",
       "4     2144\n",
       "6     2010\n",
       "2     1241\n",
       "19     886\n",
       "14     871\n",
       "7      813\n",
       "17     804\n",
       "10     714\n",
       "13     672\n",
       "0      468\n",
       "3      395\n",
       "1      260\n",
       "15     215\n",
       "5       16\n",
       "Name: category_label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['category_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:33.281002Z",
     "start_time": "2019-04-12T00:46:26.102691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.04142968129714935, 0.9585703187028507),\n",
       " array([ 1.55982906,  2.80769231,  0.58823529,  1.84810127,  0.34048507,\n",
       "        45.625     ,  0.36318408,  0.89790898,  0.0997404 ,  0.18042511,\n",
       "         1.02240896,  0.        ,  0.21073903,  1.08630952,  0.83811711,\n",
       "         3.39534884,  0.        ,  0.9079602 ,  0.        ,  0.82392777,\n",
       "         0.        ,  0.        ,  0.        ]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def category_classweight():\n",
    "    \"\"\"calculate the category's class weight\n",
    "    \n",
    "    Return\n",
    "        an 1D np.ndarray which index is equal to the category's index\n",
    "    \"\"\"\n",
    "    class_dict = train_df['category_label'].value_counts().to_dict()\n",
    "    num_max = max(class_dict.values())\n",
    "    class_weight = []\n",
    "    for i in range(num_category):\n",
    "        if i not in class_dict:\n",
    "            class_weight.append(0.)\n",
    "        else:\n",
    "            class_weight.append(730 / class_dict[i])\n",
    "    class_weight = np.array(class_weight)\n",
    "    return class_weight\n",
    "\n",
    "\n",
    "def attribute_classweight():\n",
    "    \"\"\"calculate the attribute's class weight\n",
    "    \n",
    "    Return\n",
    "        p_weight: the positive weight of the train_df\n",
    "        n_weight: the negative weight of the train_df\n",
    "    \"\"\"\n",
    "    p_weight = 0\n",
    "    for i in range(len(train_df)):\n",
    "        attrs = str(train_df.loc[i]['attribute_labels'])\n",
    "        p_weight += len(attrs.split(' '))\n",
    "    p_weight = p_weight / (len(train_df) * num_attr)\n",
    "    n_weight = 1 - p_weight\n",
    "    return p_weight, n_weight\n",
    "\n",
    "\n",
    "category_weight = category_classweight()\n",
    "attribute_weight = attribute_classweight(); attribute_weight, category_weight            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:33.315193Z",
     "start_time": "2019-04-12T00:46:33.285083Z"
    }
   },
   "outputs": [],
   "source": [
    "def category_loss(y_true, y_pred, gamma=0.):\n",
    "    \"\"\"the category loss using focal loss.\n",
    "    \n",
    "    The reference paper of focal loss is Focal Loss for Dense Object Detection\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 23)\n",
    "        y_pred: the predicted value, shape=(None, 23)\n",
    "        gamma: if 0, focal loss is equal to the category crossentropy loss.\n",
    "    \n",
    "    Return\n",
    "        the loss of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    loss = -y_true * K.log(y_pred + K.epsilon()) * category_weight\n",
    "    loss = K.sum(loss, axis=-1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def attribute_loss(y_true, y_pred, gamma=0.):\n",
    "    \"\"\"the attribute loss using focal loss.\n",
    "    \n",
    "    The reference paper of focal loss is Focal Loss for Dense Object Detection\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 463)\n",
    "        y_pred: the predicted value, shape=(None, 463)\n",
    "        gamma: if 0, focal loss is equal to the category crossentropy loss.\n",
    "        \n",
    "    Return\n",
    "        the loss of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    y_ = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred) + K.epsilon()\n",
    "    loss = -K.log(y_)\n",
    "\n",
    "    weight = y_true * attribute_weight[0] + (1.0 - y_true) * attribute_weight[1]\n",
    "    loss *= weight\n",
    "\n",
    "    loss = K.sum(loss, axis=-1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lands_loss(y_true, y_pred):\n",
    "    \"\"\"the landmarks loss that consists of two parts, the category crossentropy of landmarks' visibility\n",
    "    and the L2 loss of landmarks' location\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 40). And each landmark consists of the visibility's status\n",
    "            of 3 elements and the location (x/w, y/h) of 2 elements. And each image has 8 landmarks.\n",
    "        y_pred: the predicted value, shape=(None, 40).\n",
    "        \n",
    "    Return\n",
    "        the loss of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    y_true = K.reshape(y_true, (-1, 5))\n",
    "    y_pred = K.reshape(y_pred, (-1, 5))\n",
    "    \n",
    "    lands_true = y_true[:, 3:]\n",
    "    lands_pred = y_pred[:, 3:]\n",
    "    lands_gate = y_true[:, 0, None]    # only the visiable landmarks will be calculated\n",
    "    lands = K.sum((K.sigmoid(lands_pred) - lands_true) ** 2 * lands_gate, axis=-1)\n",
    "    \n",
    "    lands_v_true = y_true[:, :3]\n",
    "    lands_v_pred = y_pred[:, :3]\n",
    "    visibility = -lands_v_true * K.log(K.softmax(lands_v_pred) + K.epsilon())\n",
    "    visibility = K.sum(visibility, axis=-1)\n",
    "    \n",
    "    loss = lands + visibility\n",
    "    loss = K.sum(K.reshape(loss, (-1, 8)), axis=-1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\"the triplet loss.\n",
    "    \n",
    "    The reference paper of focal loss is In Defense of the Triplet Loss for Person Re-Identification.\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 1). In fact, the triplet loss dosen't need any target,\n",
    "            but the keras framework need.\n",
    "        y_pred: the predicted value, shape=(None, length of the output tensor)\n",
    "    \n",
    "    Return\n",
    "        the loss of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    margin = 0.3\n",
    "    loss = K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    # donot use K.max, it calculates maximum value in a tensor.\n",
    "    loss = K.maximum(0., margin + loss + 0. * y_true)\n",
    "    return loss\n",
    "\n",
    "def top1(y_true, y_pred, k=1):\n",
    "    \"\"\"top1 categorical accuracy\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 23)\n",
    "        y_pred: the predicted value, shape=(None, 23)\n",
    "        \n",
    "    Returns\n",
    "        the top1-acc of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    return K.cast(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), K.floatx())\n",
    "\n",
    "\n",
    "def top2(y_true, y_pred, k=2):\n",
    "    \"\"\"top2 categorical accuracy\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 23)\n",
    "        y_pred: the predicted value, shape=(None, 23)\n",
    "        \n",
    "    Returns\n",
    "        the top2-acc of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    return K.cast(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), K.floatx())\n",
    "\n",
    "\n",
    "def top3(y_true, y_pred, k=3):\n",
    "    \"\"\"top3 categorical accuracy\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 23)\n",
    "        y_pred: the predicted value, shape=(None, 23)\n",
    "        \n",
    "    Returns\n",
    "        the top3-acc of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    return K.cast(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), K.floatx())\n",
    "\n",
    "\n",
    "def top5(y_true, y_pred, k=5):\n",
    "    \"\"\"top5 categorical accuracy\n",
    "    \n",
    "    Arguments\n",
    "        y_true: the learning target, shape=(None, 23)\n",
    "        y_pred: the predicted value, shape=(None, 23)\n",
    "        \n",
    "    Returns\n",
    "        the top5-acc of shape=(None, 1)\n",
    "    \"\"\"\n",
    "    return K.cast(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roi pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:33.344202Z",
     "start_time": "2019-04-12T00:46:33.317760Z"
    }
   },
   "outputs": [],
   "source": [
    "class RoiPooling(Layer):\n",
    "    \"\"\"Landmark ROI pooling layer for 2D inputs.\n",
    "    \n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\n",
    "    \n",
    "    Arguments\n",
    "        pool_size: int. Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
    "        num_rois: number of regions of interest to be used\n",
    "        window_size: the slide window size\n",
    "        \n",
    "    Input shape\n",
    "        list of one 4D tensor and one 2D tensor  [X_img,X_roi] with shape:\n",
    "        X_img: (batches, channels, rows, cols) if dim_ordering='th' or 4D tensor with shape:\n",
    "            (batch, rows, cols, channels) if dim_ordering='tf'.\n",
    "        X_roi: (batches, num_rois * 5) list of rois, with ordering (x_left_top, y_left_top) in the\n",
    "            first two elements where axis=-1.\n",
    "            \n",
    "    Output shape\n",
    "        4D tensor with shape:\n",
    "            (batches, pool_size, pool_size, channels * num_rois) if dim_ordering='tf'\n",
    "            (batch, channels * num_rois, pool_size, pool_size) if dim_ordering='th'\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size=2, num_rois=8, window_size=4, **kwargs):\n",
    "        self.dim_ordering = K.image_dim_ordering()\n",
    "        assert self.dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n",
    "        self.window_size = window_size\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "        self.data_format = 'channels_last' if self.dim_ordering == 'tf' else 'channels_first'\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            self.nb_channels = input_shape[0][1]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            self.nb_channels = input_shape[0][3]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            return None, self.num_rois * self.nb_channels, self.pool_size, self.pool_size\n",
    "        else:\n",
    "            return None, self.pool_size, self.pool_size, self.nb_channels * self.num_rois\n",
    "\n",
    "    def call(self, x, mask=None): \n",
    "        assert (len(x) == 2)\n",
    "        # 输入特征，坐标信息\n",
    "        img, rois = x[0], x[1]\n",
    "        \n",
    "        # convert img\n",
    "        if self.dim_ordering == 'th':\n",
    "            img = K.permute_dimensions(img, (0, 2, 3, 1))\n",
    "        else:\n",
    "            img = K.permute_dimensions(img, (0, 1, 2, 3))\n",
    "        \n",
    "        # convert boxes\n",
    "        shape = K.int_shape(img)\n",
    "        \n",
    "        rois = K.reshape(rois, (-1, 5))[:, 3:]\n",
    "        x1 = rois[..., 0]\n",
    "        y1 = rois[..., 1]\n",
    "        x2 = rois[..., 0] + K.cast(self.window_size / shape[2], K.floatx())\n",
    "        y2 = rois[..., 1] + K.cast(self.window_size / shape[1], K.floatx())\n",
    "        \n",
    "        x1 = K.expand_dims(x1, axis=-1)\n",
    "        y1 = K.expand_dims(y1, axis=-1)\n",
    "        x2 = K.expand_dims(x2, axis=-1)\n",
    "        y2 = K.expand_dims(y2, axis=-1)\n",
    "\n",
    "        boxes = K.concatenate([y1, x1, y2, x2], axis=-1)\n",
    "        \n",
    "        # convert img\n",
    "        img = K.concatenate([img] * self.num_rois, axis=1)\n",
    "        img = K.reshape(img, (-1, ) + shape[1:])\n",
    "        \n",
    "        # crop and resize image\n",
    "        # input: img(None*8, H, W, C), boxes(None*8, 4)\n",
    "        # return: tensor(None, slice_height, slice_width, C*num_boxes)\n",
    "        box_ind = K.zeros_like(boxes, 'int32')\n",
    "        box_ind = box_ind[..., 0]\n",
    "        box_ind = K.reshape(box_ind, (-1, ))\n",
    "        \n",
    "        slices = tf.image.crop_and_resize(img, boxes, box_ind, (self.pool_size, self.pool_size))\n",
    "        slices = K.reshape(slices, (-1, self.pool_size, self.pool_size, self.num_rois * shape[-1]))\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            slices = K.permute_dimensions(slices, (0, 3, 1, 2))\n",
    "        else:\n",
    "            slices = K.permute_dimensions(slices, (0, 1, 2, 3))\n",
    "\n",
    "        return slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:39.520232Z",
     "start_time": "2019-04-12T00:46:33.346689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1/random_uniform/RandomUniform (defined at /root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4139) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'fc1/random_uniform/RandomUniform', defined at:\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1152, in inner\n    self.run()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1069, in run\n    yielded = self.gen.send(value)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 319, in wrapper\n    _futures_to_runners[future] = Runner(result, future, yielded)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1003, in __init__\n    self.run()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1069, in run\n    yielded = self.gen.send(value)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-01ad7796647e>\", line 164, in <module>\n    model_source, model_blue = build_model()\n  File \"<ipython-input-17-01ad7796647e>\", line 25, in build_model\n    model = VGG16(input_tensor=inp_a, weights='/Fashion/FashionNet/src/weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/applications/__init__.py\", line 28, in wrapper\n    return base_fun(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/applications/vgg16.py\", line 11, in VGG16\n    return vgg16.VGG16(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras_applications/vgg16.py\", line 178, in VGG16\n    x = layers.Dense(4096, activation='relu', name='fc1')(x)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py\", line 247, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 777, in random_uniform\n    name=name)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1/random_uniform/RandomUniform (defined at /root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4139) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node fc1/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-01ad7796647e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mmodel_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_blue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-01ad7796647e>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inp_n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Fashion/FashionNet/src/weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# if using model.trainable = False, the block5 will be untrainable too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras_applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mkeras_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                              ' elements.')\n\u001b[1;32m   1057\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1/random_uniform/RandomUniform (defined at /root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4139) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'fc1/random_uniform/RandomUniform', defined at:\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1152, in inner\n    self.run()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1069, in run\n    yielded = self.gen.send(value)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 319, in wrapper\n    _futures_to_runners[future] = Runner(result, future, yielded)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1003, in __init__\n    self.run()\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 1069, in run\n    yielded = self.gen.send(value)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tornado/gen.py\", line 307, in wrapper\n    yielded = next(result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-01ad7796647e>\", line 164, in <module>\n    model_source, model_blue = build_model()\n  File \"<ipython-input-17-01ad7796647e>\", line 25, in build_model\n    model = VGG16(input_tensor=inp_a, weights='/Fashion/FashionNet/src/weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/applications/__init__.py\", line 28, in wrapper\n    return base_fun(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/applications/vgg16.py\", line 11, in VGG16\n    return vgg16.VGG16(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras_applications/vgg16.py\", line 178, in VGG16\n    x = layers.Dense(4096, activation='relu', name='fc1')(x)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py\", line 247, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 777, in random_uniform\n    name=name)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1/random_uniform/RandomUniform (defined at /root/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4139) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    \"\"\"build the Fashion Net model\n",
    "    \n",
    "    The reference paper of focal loss is DeepFashion: Powering Robust Clothes Recognition and \n",
    "    Retrieval with Rich Annotations.\n",
    "    \n",
    "    Using batchnormalization to avoid that the output of the nn is 0.\n",
    "    \n",
    "    Arguments\n",
    "        lr: the learning rate\n",
    "    \n",
    "    Return\n",
    "        model_stage1: the first stage model for train\n",
    "        model_stage2: the second stage model for train\n",
    "        model_blue: the model for the prediction of categroy, landmarks visibility and landmarks location\n",
    "    \"\"\"\n",
    "    kwargs = {'kernel_initializer': 'he_normal', 'padding': 'same'}\n",
    "    fkwargs = {'kernel_initializer': 'he_normal'}\n",
    "    \n",
    "    # base model\n",
    "    inp_a = layers.Input(shape=image_shape, name='inp_a')\n",
    "    inp_p = layers.Input(shape=image_shape, name='inp_p')\n",
    "    inp_n = layers.Input(shape=image_shape, name='inp_n')\n",
    "    \n",
    "    model = VGG16(input_tensor=inp_a, weights='/Fashion/FashionNet/src/weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    # if using model.trainable = False, the block5 will be untrainable too\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'block4_pool':\n",
    "            break\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    node = model.get_layer(name='block4_conv3').output\n",
    "    \n",
    "    # blue\n",
    "    blue = layers.MaxPool2D((2, 2), strides=(2, 2), name='blue_pool1')(node)\n",
    "    blue = layers.Conv2D(512, (3, 3), activation='relu', name='blue_conv1', **kwargs)(blue)\n",
    "    blue = layers.Conv2D(512, (3, 3), activation='relu', name='blue_conv2', **kwargs)(blue)\n",
    "    blue = layers.Conv2D(512, (3, 3), activation='relu', name='blue_conv3', **kwargs)(blue)\n",
    "    blue = layers.MaxPool2D((2, 2), strides=(2, 2), name='blue_pool')(blue)   \n",
    "    \n",
    "    blue = layers.Flatten(name='blue_flatten')(blue)\n",
    "    \n",
    "    blue = layers.Dense(4096, name='blue_fc1', **fkwargs)(blue)\n",
    "    blue = layers.BatchNormalization(name='blue_normal1')(blue)\n",
    "    blue = layers.Activation('relu', name='blue_fc1_activation')(blue)\n",
    "    blue = layers.Dense(4096, name='blue_fc2', **fkwargs)(blue)\n",
    "    blue = layers.BatchNormalization(name='blue_normal2')(blue)\n",
    "    blue = layers.Activation('relu', name='blue_fc2_activation')(blue)\n",
    "    ## land marks and category loss\n",
    "    blue_cates = layers.Dense(\n",
    "        num_category,\n",
    "        kernel_initializer='he_normal',\n",
    "        activation='softmax',\n",
    "        name='blue_cates'\n",
    "    )(blue)\n",
    "    blue_lands = layers.Dense(8 * (3 + 2), kernel_initializer='he_normal', name='blue_lands')(blue)\n",
    "    model_blue = Model(inp_a, [blue_cates, blue_lands], name='model_blue')\n",
    "    \n",
    "    # red\n",
    "    red = layers.MaxPool2D((2, 2), strides=(2, 2), name='red_pool1')(node)\n",
    "    red = layers.Conv2D(512, (3, 3), activation='relu', name='red_conv1', **kwargs)(red)\n",
    "    red = layers.Conv2D(512, (3, 3), activation='relu', name='red_conv2', **kwargs)(red)\n",
    "    red = layers.Conv2D(512, (3, 3), activation='relu', name='red_conv3', **kwargs)(red)\n",
    "    red = layers.MaxPooling2D((2, 2), strides=(2, 2), name='red_pool')(red)\n",
    "    red = layers.Flatten(name='red_flatten')(red)\n",
    "    red = layers.Dense(4096, name='red_fc1', **fkwargs)(red)\n",
    "    \n",
    "    # green\n",
    "    green = RoiPooling(pool_size=2, num_rois=8, window_size=4, name='green_roi')([node, blue_lands])\n",
    "    green = layers.Flatten(name='green_flatten')(green)\n",
    "    green = layers.Dense(1024, name='green_fc1', **fkwargs)(green)\n",
    "    \n",
    "    # concate the red and green\n",
    "    red_green = layers.concatenate([red, green], name='concate')\n",
    "    red_green = layers.BatchNormalization(name='concate_normal')(red_green)\n",
    "    red_green = layers.Activation('relu', name='concate_activation')(red_green)\n",
    "    ## concate category loss\n",
    "    red_green_cates = layers.Dense(\n",
    "        num_category,\n",
    "        kernel_initializer='he_normal',\n",
    "        activation='softmax',\n",
    "        name='concate_category'\n",
    "    )(red_green)\n",
    "    ## attribute loss\n",
    "    red_green_attrs = layers.Dense(\n",
    "        num_attr,\n",
    "        kernel_initializer='he_normal',\n",
    "        activation='sigmoid',\n",
    "        name='attribute'\n",
    "    )(red_green)\n",
    "      \n",
    "    model_source = Model(inp_a, [blue_cates, blue_lands, red_green_cates, red_green_attrs])\n",
    "    \n",
    "    # turning weights\n",
    "    for layer in model.layers:\n",
    "        name = layer.name\n",
    "        if 'block5' in name:\n",
    "            blue_name = \"blue_{0}\".format(name.split('_')[1])\n",
    "            model_source.get_layer(blue_name).set_weights(layer.get_weights())\n",
    "            red_name = \"red_{0}\".format(name.split('_')[1])\n",
    "            model_source.get_layer(red_name).set_weights(layer.get_weights())\n",
    "\n",
    "    return model_source, model_blue\n",
    "    \n",
    "\n",
    "def change_stage(model, lr=3e-4, stage=1):\n",
    "    \"\"\"change the model's training stage\n",
    "    \n",
    "    Arguments\n",
    "        model: the model object\n",
    "        lr: learning rate\n",
    "        stage: if 1, blue model will be trainable, if 2, blue model will be untrainable\n",
    "    \"\"\"\n",
    "    opt = Adam(lr)\n",
    "    # stage1: train the blue node\n",
    "    if stage == 1:\n",
    "        for layer in model.layers:\n",
    "            if 'blue' in layer.name:\n",
    "                layer.trainable = True \n",
    "        model.compile(\n",
    "            opt,\n",
    "            loss={\n",
    "                'blue_cates': category_loss,\n",
    "                'blue_lands': lands_loss,\n",
    "                'concate_category': category_loss,\n",
    "                'attribute': attribute_loss,\n",
    "            },\n",
    "            loss_weights={\n",
    "                'blue_cates': 1.,\n",
    "                'blue_lands': 1.,\n",
    "                'concate_category': 0.1,\n",
    "                'attribute': 0.1,\n",
    "            },\n",
    "            metrics={\n",
    "                'blue_cates': [top1, top2, top3, top5]\n",
    "            }\n",
    "        )\n",
    "    # stage2: train the red and green node\n",
    "    elif stage == 2:\n",
    "        for layer in model.layers:\n",
    "            if 'blue' in layer.name:\n",
    "                layer.trainable = False\n",
    "                \n",
    "        model.compile(\n",
    "            opt,\n",
    "            loss={\n",
    "                'blue_cates': category_loss,\n",
    "                'blue_lands': lands_loss,\n",
    "                'concate_category': category_loss,\n",
    "                'attribute': attribute_loss\n",
    "            },\n",
    "            loss_weights={\n",
    "                'blue_cates': 0.,\n",
    "                'blue_lands': 0.,\n",
    "                'concate_category': 1.,\n",
    "                'attribute': 1.\n",
    "            },\n",
    "            metrics={\n",
    "                'concate_category': [top1, top2, top3, top5]\n",
    "            }\n",
    "        )\n",
    "    return model\n",
    "    \n",
    "model_source, model_blue = build_model()\n",
    "model = change_stage(model_source); model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:39.544850Z",
     "start_time": "2019-04-12T00:46:39.523261Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRFinder(object):\n",
    "    \"\"\"Plots the change of the loss function of a Keras model when the learning rate is exponentially\n",
    "    increasing. See for details: \n",
    "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # Log the learning rate\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if math.isnan(loss) or loss > self.best_loss * 4:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, data_gen, start_lr, end_lr, epochs=1, class_weight=None):\n",
    "        num_batches = epochs * len(data_gen)\n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
    "\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit_generator(data_gen, epochs=epochs, callbacks=[callback], class_weight=class_weight)\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "        \"\"\"\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        Plots rate of change of the loss function.\n",
    "        Parameters:\n",
    "            sma - number of batches for simple moving average to smooth out the curve.\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            y_lim - limits for the y axis.\n",
    "        \"\"\"\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)\n",
    "    \n",
    "    def plot_loss_smoothing(self, smoothing=0.6, n_skip_beginning=10, n_skip_end=5):\n",
    "        \"\"\"plot smoothing as like tensorboard\n",
    "        \"\"\"\n",
    "        losses, lrs = self.losses[n_skip_beginning:-n_skip_end], self.lrs[n_skip_beginning:-n_skip_end]\n",
    "        last = losses[0]\n",
    "        smoothes = []\n",
    "        for point in losses:\n",
    "            smooth = last * smoothing + (1-smoothing) * point\n",
    "            smoothes.append(smooth)\n",
    "            last = point\n",
    "        \n",
    "        plt.ylabel(\"rate of loss smoothing\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(lrs, smoothes)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:39.570926Z",
     "start_time": "2019-04-12T00:46:39.548754Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_lr(model, lr):\n",
    "    \"\"\"config the model's learning rate\"\"\"\n",
    "    K.set_value(model.optimizer.lr, float(lr))\n",
    "    \n",
    "    \n",
    "def get_lr(model):\n",
    "    \"\"\"get the model's learning rate\"\"\"\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    \"\"\"save the model of current step into the MODEL_PATH\"\"\"\n",
    "#     model.save(f'{MODEL_PATH}epoch{steps}.h5')\n",
    "    model.save('{0}epoch{1}.h5'.format(MODEL_PATH,steps))\n",
    "    \n",
    "    \n",
    "def load_model(model, temp_step):\n",
    "    \"\"\"load model from what the function save_model saved\n",
    "    \n",
    "    Arguments\n",
    "        model: the model have been built\n",
    "        temp_step: which step of the model you want to load\n",
    "    \n",
    "    Return\n",
    "        the loaded model\n",
    "    \"\"\"\n",
    "    global steps\n",
    "    steps = temp_step\n",
    "    \n",
    "    model_file = '{0}epoch{1}.h5'.format(MODEL_PATH,steps) #f'{MODEL_PATH}epoch{steps}.h5'\n",
    "    tmp = keras.models.load_model(model_file, compile=False)\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    return model\n",
    "    \n",
    "\n",
    "def load_modelfile(model, modelfile):\n",
    "    \"\"\"load the model from model path\n",
    "    \n",
    "    Arguments\n",
    "        model: the model have been built\n",
    "        modelfile: the model file path, such as '../data/output/models/fashionnet/epoch.04-0.22-0.333.h5' or\n",
    "            'epoch.04-0.22-0.333.h5'\n",
    "    \n",
    "    Return\n",
    "        the loaded model\n",
    "    \"\"\"\n",
    "    global steps\n",
    "    steps = int(modelfile.split('-')[0].split('.')[-1])\n",
    "    \n",
    "    model_file = '{0}{1}'.format(MODEL_PATH,modelfile)#f'{MODEL_PATH}{modelfile}'\n",
    "#     tmp = keras.models.load_model(model_file, compile=False, custom_objects={\n",
    "#         'RoiPooling': RoiPooling\n",
    "#     })\n",
    "#     model.set_weights(tmp.get_weights())\n",
    "    model.load_weights(model_file, by_name=True)\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def callbacks():\n",
    "    \"\"\"get the configured keras train callbacks\"\"\"\n",
    "    ckpt_file = MODEL_PATH + 'ckpt_model.{epoch:02d}-{val_loss:.4f}-{val_blue_cates_loss:.4f}.h5'\n",
    "    checkpoint = ModelCheckpoint(ckpt_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    \n",
    "#     csv_log_file = f'{LOGGER_PATH}{model_dict[no_model]}.csv'\n",
    "    csv_log_file='{0}{1}.csv'.format(LOGGER_PATH,model_dict[no_model])\n",
    "    csv_log = CSVLogger(csv_log_file)\n",
    "    \n",
    "#     tensorboard = TensorBoard(f'{LOGGER_PATH}')\n",
    "    tensorboard = TensorBoard(LOGGER_PATH)\n",
    "    \n",
    "    return [checkpoint, csv_log, tensorboard]\n",
    "\n",
    "\n",
    "def make_steps(step):\n",
    "    \"\"\"training some epoches\n",
    "    \n",
    "    Arguments\n",
    "        step: how many epoches the model will be trained \n",
    "    \"\"\"\n",
    "    global steps, histories\n",
    "\n",
    "    # Train the model for 'step' epochs\n",
    "    history = model.fit_generator(\n",
    "        TrainSequence(train_df, batch_size=64),\n",
    "        validation_data=ValSequence(val_df, batch_size=64),\n",
    "        callbacks=callbacks(),\n",
    "        initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=8, verbose=1).history\n",
    "    steps += step\n",
    "\n",
    "    # Collect history data\n",
    "    history['epochs'] = steps\n",
    "    history['lr'] = get_lr(model)\n",
    "    print(history['epochs'], history['lr'])\n",
    "    histories.append(history)\n",
    "    \n",
    "    \n",
    "def find_lr(model, start_lr=3e-5, end_lr=1):\n",
    "    \"\"\"plot the loss-lr curve to find an appropriate model\"\"\"\n",
    "    lr_finder = LRFinder(model)\n",
    "    lr_finder.find(\n",
    "        TrainSequence(train_df, batch_size=8),\n",
    "        start_lr=start_lr, end_lr=end_lr\n",
    "    )\n",
    "    lr_finder.plot_loss()\n",
    "    return lr_finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StartTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T00:46:40.693019Z",
     "start_time": "2019-04-12T00:46:40.688404Z"
    }
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到上阶段最佳模型，使用第1阶段模型继续训练。如果一开始训练，准确率就高达99%，那么多半是初始化异常，重新启动初始化就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-11T08:28:00.870Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = change_stage(model, lr=1e-4, stage=1)\n",
    "# make_steps(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到上阶段最佳模型，使用第2阶段模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:06:54.971042Z",
     "start_time": "2019-04-11T10:06:53.525348Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_modelfile(model, 'ckpt_model.02-5.0849-0.8805.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:59:51.202914Z",
     "start_time": "2019-04-11T10:07:36.391014Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.5/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 3/4\n",
      "412/412 [==============================] - 380s 922ms/step - loss: 5.3955 - blue_cates_loss: 0.6836 - blue_lands_loss: 4.3889 - concate_category_loss: 0.8000 - attribute_loss: 4.5955 - concate_category_top1: 0.4346 - concate_category_top2: 0.6571 - concate_category_top3: 0.7761 - concate_category_top5: 0.9003 - val_loss: 5.4401 - val_blue_cates_loss: 0.8805 - val_blue_lands_loss: 3.6375 - val_concate_category_loss: 0.8962 - val_attribute_loss: 4.5440 - val_concate_category_top1: 0.4778 - val_concate_category_top2: 0.6986 - val_concate_category_top3: 0.8157 - val_concate_category_top5: 0.9257\n",
      "\n",
      "Epoch 00003: val_loss improved from inf to 5.44013, saving model to ../data/output/models/fashionnet/ckpt_model.03-5.4401-0.8805.h5\n",
      "Epoch 4/4\n",
      "412/412 [==============================] - 368s 894ms/step - loss: 5.1559 - blue_cates_loss: 0.6701 - blue_lands_loss: 4.4100 - concate_category_loss: 0.6694 - attribute_loss: 4.4865 - concate_category_top1: 0.4859 - concate_category_top2: 0.6992 - concate_category_top3: 0.8125 - concate_category_top5: 0.9194 - val_loss: 5.4435 - val_blue_cates_loss: 0.8805 - val_blue_lands_loss: 3.6375 - val_concate_category_loss: 0.9064 - val_attribute_loss: 4.5370 - val_concate_category_top1: 0.4521 - val_concate_category_top2: 0.6657 - val_concate_category_top3: 0.7930 - val_concate_category_top5: 0.9145\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 5.44013\n",
      "4 1e-04\n"
     ]
    }
   ],
   "source": [
    "model = change_stage(model, lr=1e-4, stage=2)\n",
    "make_steps(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步降低lr，并重新训练第一阶段模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:05:10.068880Z",
     "start_time": "2019-04-11T11:05:07.384128Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_modelfile(model, 'ckpt_model.03-5.4401-0.8805.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:54:06.703997Z",
     "start_time": "2019-04-11T11:05:11.390550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.5/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,224,224,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node block1_conv1/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics_3/top1/Mean}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2284f9ff6ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmake_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-34664c9f7c83>\u001b[0m in \u001b[0;36mmake_steps\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mValSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=8, verbose=1).history\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,224,224,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node block1_conv1/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics_3/top1/Mean}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "model = change_stage(model, lr=3e-5, stage=1)\n",
    "make_steps(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T01:27:43.040772Z",
     "start_time": "2019-04-12T01:27:41.856142Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = load_modelfile(model, 'ckpt_model.14-4.1022-0.8233.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T02:16:56.703077Z",
     "start_time": "2019-04-12T01:27:51.608732Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = change_stage(model, lr=1e-5, stage=2)\n",
    "# make_steps(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:37:07.346391Z",
     "start_time": "2019-03-26T02:36:31.689237Z"
    }
   },
   "source": [
    "重复以上2个步骤，直到模型最终拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.5/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainSequence' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-02edce5a0dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#显示每一个测试集各个类别的概率，这个值的shape为（10000,10）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainSequence' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "# predictions = model.predict(TrainSequence(test_df, batch_size=64)) #显示每一个测试集各个类别的概率，这个值的shape为（10000,10）\n",
    "# print(predictions)\n",
    "# print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
